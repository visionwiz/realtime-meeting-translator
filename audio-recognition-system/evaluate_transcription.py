#!/usr/bin/env python3
"""
éŸ³å£°èªè­˜ç²¾åº¦è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
æ­£è§£ãƒ‡ãƒ¼ã‚¿ã¨éŸ³å£°èªè­˜çµæœã‚’æ¯”è¼ƒã—ã¦WERï¼ˆWord Error Rateï¼‰ã‚’è¨ˆç®—
"""

import re
import argparse
from pathlib import Path
from typing import List, Tuple
import difflib

def parse_transcript_file(file_path: str) -> List[Tuple[str, str]]:
    """
    éŸ³å£°èªè­˜çµæœãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯æ­£è§£ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æ
    Returns: [(timestamp, text), ...]
    """
    results = []
    with open(file_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()
    
    current_timestamp = None
    for line in lines:
        line = line.strip()
        
        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—è¡Œã®æ¤œå‡º
        timestamp_match = re.match(r'\[(\d{2}:\d{2}:\d{2})\]', line)
        if timestamp_match:
            # æ­£è§£ãƒ‡ãƒ¼ã‚¿å½¢å¼: [HH:MM:SS] ãƒ†ã‚­ã‚¹ãƒˆ
            if ' ' in line:
                timestamp = timestamp_match.group(1)
                text = line[timestamp_match.end():].strip()
                results.append((timestamp, text))
            else:
                # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼: [HH:MM:SS] ã ã‘ã®è¡Œ
                current_timestamp = timestamp_match.group(1)
        
        # éŸ³å£°èªè­˜çµæœã®æ¤œå‡º
        elif line.startswith('èªè­˜çµæœ(') and current_timestamp:
            # èªè­˜çµæœ(ja): ãƒ†ã‚­ã‚¹ãƒˆ å½¢å¼
            text_match = re.match(r'èªè­˜çµæœ\([^)]+\):\s*(.+)', line)
            if text_match:
                text = text_match.group(1)
                results.append((current_timestamp, text))
                current_timestamp = None
    
    return results

def normalize_text(text: str) -> str:
    """ãƒ†ã‚­ã‚¹ãƒˆæ­£è¦åŒ–ï¼ˆæ¯”è¼ƒç”¨ï¼‰"""
    # å¥èª­ç‚¹é™¤å»ã€ã‚¹ãƒšãƒ¼ã‚¹æ­£è¦åŒ–
    text = re.sub(r'[ã€‚ã€ï¼ï¼Œ]', '', text)
    text = re.sub(r'\s+', '', text)
    return text.lower()

def calculate_wer(reference: str, hypothesis: str) -> float:
    """Word Error Rate (WER) è¨ˆç®—"""
    ref_words = list(normalize_text(reference))
    hyp_words = list(normalize_text(hypothesis))
    
    # ç·¨é›†è·é›¢è¨ˆç®—
    d = [[0 for _ in range(len(hyp_words) + 1)] for _ in range(len(ref_words) + 1)]
    
    for i in range(len(ref_words) + 1):
        d[i][0] = i
    for j in range(len(hyp_words) + 1):
        d[0][j] = j
    
    for i in range(1, len(ref_words) + 1):
        for j in range(1, len(hyp_words) + 1):
            if ref_words[i-1] == hyp_words[j-1]:
                d[i][j] = d[i-1][j-1]
            else:
                d[i][j] = min(
                    d[i-1][j] + 1,      # å‰Šé™¤
                    d[i][j-1] + 1,      # æŒ¿å…¥
                    d[i-1][j-1] + 1     # ç½®æ›
                )
    
    # WERè¨ˆç®—
    edit_distance = d[len(ref_words)][len(hyp_words)]
    wer = edit_distance / len(ref_words) if len(ref_words) > 0 else 0
    return wer

def normalize_timestamp_to_seconds(timestamp_str: str, base_time: int = 0) -> int:
    """ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ç§’ã«æ­£è¦åŒ–ï¼ˆç•°ãªã‚‹å½¢å¼å¯¾å¿œï¼‰"""
    parts = timestamp_str.split(':')
    if len(parts) == 3:
        hours, minutes, seconds = map(int, parts)
        total_seconds = hours * 3600 + minutes * 60 + seconds
        
        # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«å†…çµŒéæ™‚é–“å½¢å¼ã®å ´åˆï¼ˆ00:xx:xxï¼‰
        if hours == 0 and minutes < 10:
            return total_seconds
        
        # å®Ÿéš›ã®æ™‚åˆ»å½¢å¼ã®å ´åˆï¼ˆHH:MM:SSï¼‰- base_timeã‹ã‚‰ã®çµŒéæ™‚é–“ã‚’è¨ˆç®—
        if base_time > 0:
            return total_seconds - base_time
        
        return total_seconds
    return 0

def estimate_audio_start_time(hyp_data: List[Tuple[str, str]]) -> int:
    """éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«é–‹å§‹æ™‚åˆ»ã‚’æ¨å®šï¼ˆèªè­˜çµæœã®æœ€åˆã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‹ã‚‰ï¼‰"""
    if not hyp_data:
        return 0
    
    first_timestamp = hyp_data[0][0]
    parts = first_timestamp.split(':')
    if len(parts) == 3:
        hours, minutes, seconds = map(int, parts)
        # å®Ÿéš›ã®æ™‚åˆ»å½¢å¼ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆæ™‚é–“ãŒ1ä»¥ä¸Šã¾ãŸã¯åˆ†ãŒ10ä»¥ä¸Šï¼‰
        if hours > 0 or minutes >= 10:
            return hours * 3600 + minutes * 60 + seconds
    
    return 0

def text_similarity(text1: str, text2: str) -> float:
    """ãƒ†ã‚­ã‚¹ãƒˆé¡ä¼¼åº¦ã‚’è¨ˆç®—ï¼ˆ0-1ã®ç¯„å›²ï¼‰"""
    # æ–‡å­—å˜ä½ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—
    matcher = difflib.SequenceMatcher(None, text1, text2)
    return matcher.ratio()

def find_optimal_matches(ref_data_sorted: List, hyp_data_sorted: List, time_threshold: int = 45) -> List[Tuple]:
    """æœ€é©ãªãƒãƒƒãƒãƒ³ã‚°ã‚’è¦‹ã¤ã‘ã‚‹ï¼ˆãƒ†ã‚­ã‚¹ãƒˆé¡ä¼¼åº¦é‡è¦–ï¼‰"""
    matches = []
    used_hyp_indices = set()
    
    for i, (ref_seconds, ref_ts, ref_text) in enumerate(ref_data_sorted):
        best_match = None
        best_text_sim = -1
        best_hyp_index = -1
        
        for j, (hyp_seconds, hyp_ts, hyp_text) in enumerate(hyp_data_sorted):
            if j in used_hyp_indices:
                continue
            
            # æ™‚é–“å·®ï¼ˆå‚è€ƒã®ã¿ - å¤§å¹…ãªãšã‚Œã¯é™¤å¤–ï¼‰
            time_diff = abs(ref_seconds - hyp_seconds)
            if time_diff > time_threshold:  # 45ç§’ä»¥ä¸Šã®ãšã‚Œã¯é™¤å¤–
                continue
            
            # ãƒ†ã‚­ã‚¹ãƒˆé¡ä¼¼åº¦ï¼ˆä¸»è¦æŒ‡æ¨™ï¼‰
            text_sim = text_similarity(ref_text, hyp_text)
            
            # é•·æ–‡ã®å ´åˆã®éƒ¨åˆ†ãƒãƒƒãƒãƒ³ã‚°ã‚‚è€ƒæ…®
            partial_sim = 0
            if len(ref_text) > 30 or len(hyp_text) > 30:  # é•·æ–‡ã®å ´åˆ
                # éƒ¨åˆ†çš„ãªä¸€è‡´ã‚‚ãƒã‚§ãƒƒã‚¯
                ref_words = ref_text.split()
                hyp_words = hyp_text.split()
                common_words = set(ref_words) & set(hyp_words)
                if ref_words and hyp_words:
                    partial_sim = len(common_words) / max(len(ref_words), len(hyp_words))
                text_sim = max(text_sim, partial_sim * 0.8)  # éƒ¨åˆ†ãƒãƒƒãƒã¯80%æ›ã‘
            
            # æœ€é«˜ã®ãƒ†ã‚­ã‚¹ãƒˆé¡ä¼¼åº¦ã‚’æŒã¤ãƒãƒƒãƒã‚’é¸æŠ
            if text_sim > best_text_sim:
                best_text_sim = text_sim
                best_match = (hyp_seconds, hyp_ts, hyp_text, time_diff, text_sim)
                best_hyp_index = j
        
        # ãƒãƒƒãƒãƒ³ã‚°åˆ¤å®š: ãƒ†ã‚­ã‚¹ãƒˆé¡ä¼¼åº¦ã®ã¿
        # é¡ä¼¼åº¦0.3ä»¥ä¸Šã§ãƒãƒƒãƒ
        min_threshold = 0.3
        
        if best_match and best_text_sim >= min_threshold:
            matches.append((
                i, ref_seconds, ref_ts, ref_text,
                best_hyp_index, best_match[0], best_match[1], best_match[2],
                best_match[3], best_match[4]  # combined_scoreå‰Šé™¤
            ))
            used_hyp_indices.add(best_hyp_index)
        else:
            # ãƒãƒƒãƒãªã—
            matches.append((
                i, ref_seconds, ref_ts, ref_text,
                -1, -1, "", "", -1, -1  # combined_scoreå‰Šé™¤
            ))
    
    return matches

def compare_transcriptions(reference_file: str, hypothesis_file: str, verbose: bool = False):
    """éŸ³å£°èªè­˜çµæœã¨æ­£è§£ãƒ‡ãƒ¼ã‚¿ã‚’æ¯”è¼ƒï¼ˆãƒ†ã‚­ã‚¹ãƒˆèªè­˜ç²¾åº¦é‡è¦–ç‰ˆï¼‰"""
    print(f"ğŸ“Š éŸ³å£°èªè­˜ç²¾åº¦è©•ä¾¡ï¼ˆãƒ†ã‚­ã‚¹ãƒˆèªè­˜ç²¾åº¦é‡è¦–ï¼‰")
    print(f"æ­£è§£ãƒ‡ãƒ¼ã‚¿: {reference_file}")
    print(f"èªè­˜çµæœ: {hypothesis_file}")
    print("=" * 60)
    
    # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
    ref_data = parse_transcript_file(reference_file)
    hyp_data = parse_transcript_file(hypothesis_file)
    
    print(f"æ­£è§£ãƒ‡ãƒ¼ã‚¿ç™ºè©±æ•°: {len(ref_data)}")
    print(f"èªè­˜çµæœç™ºè©±æ•°: {len(hyp_data)}")
    
    # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«é–‹å§‹æ™‚åˆ»ã‚’æ¨å®š
    base_time = estimate_audio_start_time(hyp_data)
    if base_time > 0:
        base_hours = base_time // 3600
        base_minutes = (base_time % 3600) // 60
        base_seconds = base_time % 60
        print(f"æ¨å®šéŸ³å£°é–‹å§‹æ™‚åˆ»: {base_hours:02d}:{base_minutes:02d}:{base_seconds:02d}")
    else:
        print("ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—å½¢å¼: éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«å†…çµŒéæ™‚é–“")
    
    print("-" * 60)
    
    # ãƒ‡ãƒ¼ã‚¿ã‚’æ™‚ç³»åˆ—é †ã«ã‚½ãƒ¼ãƒˆ
    hyp_data_sorted = []
    for hyp_ts, hyp_text in hyp_data:
        hyp_seconds = normalize_timestamp_to_seconds(hyp_ts, base_time)
        hyp_data_sorted.append((hyp_seconds, hyp_ts, hyp_text))
    hyp_data_sorted.sort(key=lambda x: x[0])
    
    ref_data_sorted = []
    for ref_ts, ref_text in ref_data:
        ref_seconds = normalize_timestamp_to_seconds(ref_ts)
        ref_data_sorted.append((ref_seconds, ref_ts, ref_text))
    ref_data_sorted.sort(key=lambda x: x[0])
    
    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±
    if verbose:
        print("\nğŸ” ãƒ‡ãƒãƒƒã‚°æƒ…å ±:")
        print("æ­£è§£ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³:")
        for ref_seconds, ref_ts, ref_text in ref_data_sorted:
            print(f"  {ref_seconds:3d}ç§’ [{ref_ts}] {ref_text[:50]}...")
        print("\nèªè­˜çµæœã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³:")
        for hyp_seconds, hyp_ts, hyp_text in hyp_data_sorted:
            print(f"  {hyp_seconds:3d}ç§’ [{hyp_ts}] {hyp_text[:50]}...")
        print("")
    
    # æœ€é©ãƒãƒƒãƒãƒ³ã‚°ã‚’å®Ÿè¡Œï¼ˆãƒ†ã‚­ã‚¹ãƒˆé¡ä¼¼åº¦é‡è¦–ï¼‰
    matches = find_optimal_matches(ref_data_sorted, hyp_data_sorted, time_threshold=45)
    
    # çµæœã‚’è¡¨ç¤º
    total_wer = 0
    matched_pairs = 0
    
    for match in matches:
        (ref_idx, ref_seconds, ref_ts, ref_text,
         hyp_idx, hyp_seconds, hyp_ts, hyp_text,
         time_diff, text_sim) = match  # combined_scoreå‰Šé™¤
        
        if hyp_idx >= 0:  # ãƒãƒƒãƒã—ãŸå ´åˆ
            wer = calculate_wer(ref_text, hyp_text)
            total_wer += wer
            matched_pairs += 1
            
            if verbose or wer > 0.3:  # è©³ç´°è¡¨ç¤ºã¾ãŸã¯é«˜ã‚¨ãƒ©ãƒ¼ç‡
                print(f"\n[{ref_ts}] ç™ºè©± {ref_idx+1} â†’ [{hyp_ts}]")
                print(f"æ­£è§£: {ref_text}")
                print(f"èªè­˜: {hyp_text}")
                print(f"WER: {wer:.3f} | é¡ä¼¼åº¦: {text_sim:.3f} ({'âœ…' if wer < 0.1 else 'âš ï¸' if wer < 0.3 else 'âŒ'})")
                if verbose and abs(time_diff) > 10:
                    print(f"å‚è€ƒ: æ™‚å·® {time_diff:.0f}ç§’")
                
                if verbose:
                    # è©³ç´°å·®åˆ†è¡¨ç¤º
                    diff = list(difflib.unified_diff(
                        [ref_text], [hyp_text], 
                        fromfile='æ­£è§£', tofile='èªè­˜', lineterm=''
                    ))
                    if len(diff) > 2:
                        for line in diff[2:]:
                            print(f"  {line}")
            elif wer < 0.3:  # è‰¯å¥½ãªçµæœã¯ç°¡æ½”ã«è¡¨ç¤º
                print(f"\n[{ref_ts}] ç™ºè©± {ref_idx+1} â†’ [{hyp_ts}] âœ… WER: {wer:.3f} | é¡ä¼¼åº¦: {text_sim:.3f}")
        else:  # ãƒãƒƒãƒã—ãªã‹ã£ãŸå ´åˆ
            print(f"\n[{ref_ts}] ç™ºè©± {ref_idx+1} - âŒ å¯¾å¿œã™ã‚‹èªè­˜çµæœãªã—")
            print(f"æ­£è§£: {ref_text}")
    
    # ä½¿ç”¨ã•ã‚Œãªã‹ã£ãŸèªè­˜çµæœã‚’ãƒã‚§ãƒƒã‚¯
    used_hyp_indices = {match[4] for match in matches if match[4] >= 0}
    unused_hyp = []
    for j, (hyp_seconds, hyp_ts, hyp_text) in enumerate(hyp_data_sorted):
        if j not in used_hyp_indices:
            unused_hyp.append((hyp_ts, hyp_text))
    
    if unused_hyp and verbose:
        print(f"\nğŸ“‹ ãƒãƒƒãƒã—ãªã‹ã£ãŸèªè­˜çµæœ ({len(unused_hyp)}ä»¶):")
        for hyp_ts, hyp_text in unused_hyp:
            print(f"  [{hyp_ts}] {hyp_text}")
    
    # çµ±è¨ˆè¡¨ç¤º
    print("\n" + "=" * 60)
    if matched_pairs > 0:
        avg_wer = total_wer / matched_pairs
        accuracy = max(0, (1 - avg_wer) * 100)  # è² ã®å€¤ã‚’é˜²ã
        match_rate = (matched_pairs / len(ref_data)) * 100
        
        print(f"ğŸ“ˆ éŸ³å£°èªè­˜ç²¾åº¦è©•ä¾¡çµæœ:")
        print(f"  ãƒãƒƒãƒã—ãŸç™ºè©±: {matched_pairs}/{len(ref_data)} ({match_rate:.1f}%)")
        print(f"  å¹³å‡WER: {avg_wer:.3f}")
        print(f"  éŸ³å£°èªè­˜ç²¾åº¦: {accuracy:.1f}%")
        
        # è©•ä¾¡åˆ¤å®šï¼ˆãƒ†ã‚­ã‚¹ãƒˆèªè­˜ç²¾åº¦é‡è¦–ï¼‰
        if match_rate >= 90 and accuracy >= 95:
            print(f"  ç·åˆåˆ¤å®š: âœ… å„ªç§€ - é«˜ç²¾åº¦ãªéŸ³å£°èªè­˜")
        elif match_rate >= 80 and accuracy >= 90:
            print(f"  ç·åˆåˆ¤å®š: ğŸŸ¢ è‰¯å¥½ - å®Ÿç”¨çš„ãªéŸ³å£°èªè­˜")
        elif match_rate >= 70 and accuracy >= 80:
            print(f"  ç·åˆåˆ¤å®š: ğŸŸ¡ æ™®é€š - æ”¹å–„ã®ä½™åœ°ã‚ã‚Š")
        else:
            print(f"  ç·åˆåˆ¤å®š: ğŸ”´ è¦æ”¹å–„ - éŸ³å£°èªè­˜ç²¾åº¦ã«èª²é¡Œ")
            
        # æ”¹å–„ææ¡ˆï¼ˆãƒ†ã‚­ã‚¹ãƒˆèªè­˜é‡è¦–ï¼‰
        if match_rate < 80:
            print(f"\nğŸ’¡ æ”¹å–„ææ¡ˆ:")
            print(f"  - ãƒãƒƒãƒç‡ãŒä½ã„ã§ã™ã€‚éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã¨æ­£è§£ãƒ‡ãƒ¼ã‚¿ã®å†…å®¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
            print(f"  - é¡ä¼¼ã—ãŸç™ºè©±ãŒè¤‡æ•°ã‚ã‚‹å ´åˆã€æ­£è§£ãƒ‡ãƒ¼ã‚¿ã®ç²’åº¦ã‚’èª¿æ•´ã—ã¦ãã ã•ã„")
        elif accuracy < 80:
            print(f"\nğŸ’¡ æ”¹å–„ææ¡ˆ:")
            print(f"  - éŸ³å£°èªè­˜ç²¾åº¦ãŒä½ã„ã§ã™ã€‚ä»¥ä¸‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„:")
            print(f"    â€¢ éŸ³å£°å“è³ªï¼ˆé›‘éŸ³ã€éŸ³é‡ã€æ˜ç­ã•ï¼‰")
            print(f"    â€¢ ãƒã‚¤ã‚¯ã®è¨­å®šã‚„ç’°å¢ƒéŸ³ã®é™¤å»")
            print(f"    â€¢ éŸ³å£°èªè­˜ãƒ¢ãƒ‡ãƒ«ã®é¸æŠ")
    else:
        print("âŒ ãƒãƒƒãƒã™ã‚‹ç™ºè©±ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        print("\nğŸ’¡ æ”¹å–„ææ¡ˆ:")
        print("  - æ­£è§£ãƒ‡ãƒ¼ã‚¿ã¨èªè­˜çµæœã®å†…å®¹ãŒå¤§ããç•°ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
        print("  - éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã¨æ­£è§£ãƒ‡ãƒ¼ã‚¿ã®å¯¾å¿œã‚’ç¢ºèªã—ã¦ãã ã•ã„")
        print("  - --verbose ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§è©³ç´°ãªå·®åˆ†ã‚’ç¢ºèªã—ã¦ãã ã•ã„")

def main():
    parser = argparse.ArgumentParser(description="éŸ³å£°èªè­˜ç²¾åº¦è©•ä¾¡")
    parser.add_argument('reference', help='æ­£è§£ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«')
    parser.add_argument('hypothesis', help='éŸ³å£°èªè­˜çµæœãƒ•ã‚¡ã‚¤ãƒ«')
    parser.add_argument('--verbose', '-v', action='store_true', help='è©³ç´°è¡¨ç¤º')
    
    args = parser.parse_args()
    
    if not Path(args.reference).exists():
        print(f"âŒ æ­£è§£ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {args.reference}")
        return
    
    if not Path(args.hypothesis).exists():
        print(f"âŒ éŸ³å£°èªè­˜çµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {args.hypothesis}")
        return
    
    compare_transcriptions(args.reference, args.hypothesis, args.verbose)

if __name__ == "__main__":
    main() 