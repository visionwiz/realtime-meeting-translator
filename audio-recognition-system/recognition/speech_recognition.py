import sys
import os
import datetime
import queue
import wave
import time
import numpy as np
import pyaudio
from language_config import LanguageConfig

if sys.platform == 'darwin':
    import mlx_whisper
    try:
        from lightning_whisper_mlx import LightningWhisperMLX
        LIGHTNING_WHISPER_AVAILABLE = True
    except ImportError:
        LIGHTNING_WHISPER_AVAILABLE = False
        print("Warning: lightning-whisper-mlx not available. Using standard mlx-whisper.")
else:
    import whisper

class SpeechRecognition:
    def __init__(self, config, processing_queue, translation_queue, args, lang_config):
        self.config = config
        self.processing_queue = processing_queue
        self.translation_queue = translation_queue
        self.args = args
        self.lang_config = lang_config

        # mlx-whisperæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
        if sys.platform == 'darwin':
            if getattr(args, 'use_lightning_whisper', False) and LIGHTNING_WHISPER_AVAILABLE:
                print("ğŸš€ Lightning Whisper MLXä½¿ç”¨ï¼ˆ10å€é«˜é€ŸåŒ–ï¼‰")
                quantization = getattr(args, 'quantization', 'none')
                quant = quantization if quantization != 'none' else None
                # Lightning Whisper MLXã§ã¯å¤šè¨€èªå¯¾å¿œãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
                model_name = "large-v3" if "large" in args.model_size else args.model_size
                
                try:
                    # é‡å­åŒ–ã®äº’æ›æ€§ãƒã‚§ãƒƒã‚¯
                    if quant is not None:
                        # ã¾ãšé‡å­åŒ–ãªã—ã§ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ã‹ã‚‰é‡å­åŒ–ã‚’è©¦è¡Œ
                        print(f"é‡å­åŒ–ï¼ˆ{quantization}ï¼‰ã‚’è©¦è¡Œä¸­...")
                        self.lightning_model = LightningWhisperMLX(
                            model=model_name,
                            batch_size=12,
                            quant=None  # æœ€åˆã¯é‡å­åŒ–ãªã—
                        )
                        # é‡å­åŒ–ã¯å¾Œã§é©ç”¨ã™ã‚‹ï¼ˆãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã§ï¼‰
                        self.quantization_mode = quantization
                        print(f"âš¡ Lightning WhisperåˆæœŸåŒ–å®Œäº† - ãƒ¢ãƒ‡ãƒ«: {args.model_size}, é‡å­åŒ–: {quantization}ï¼ˆå®Ÿè¡Œæ™‚é©ç”¨ï¼‰")
                    else:
                        self.lightning_model = LightningWhisperMLX(
                            model=model_name,
                            batch_size=12,
                            quant=None
                        )
                        self.quantization_mode = None
                        print(f"âš¡ Lightning WhisperåˆæœŸåŒ–å®Œäº† - ãƒ¢ãƒ‡ãƒ«: {args.model_size}, é‡å­åŒ–: ãªã—")
                    
                    self.use_lightning = True
                    
                except Exception as e:
                    print(f"âš ï¸ Lightning WhisperåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
                    print("æ¨™æº–MLX Whisperã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
                    self.use_lightning = False
                    self.quantization_mode = None
            else:
                print("âš¡ MLX Whisperä½¿ç”¨ï¼ˆ3-4å€é«˜é€ŸåŒ–ï¼‰")
                self.use_lightning = False
                print(f"MLX WhisperåˆæœŸåŒ–å®Œäº† - ãƒ¢ãƒ‡ãƒ«: {args.model_size}")
        else:
            self.model = whisper.load_model(self.args.model_size)
            self.use_lightning = False

        os.makedirs(self.args.output_dir, exist_ok=True)
        current_time = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        self.log_file_path = os.path.join(
            self.args.output_dir,
            f"recognized_audio_log_{self.lang_config.source_lang}_{current_time}.txt"
        )

    def recognition_thread(self, is_running):
        last_text = ""
        last_text_time = 0
        
        while is_running.is_set():
            try:
                audio_data = self.processing_queue.get(timeout=1)
                normalized_audio = self.normalize_audio(audio_data)
                
                if self.args.debug:
                    print("\néŸ³å£°èªè­˜å‡¦ç†é–‹å§‹")
                    self.save_audio_debug(audio_data, f"debug_audio_{time.time()}.wav")
                
                try:
                    if sys.platform == 'darwin':
                        if self.use_lightning:
                            # Lightning Whisper MLXï¼ˆ10å€é«˜é€ŸåŒ–ï¼‰
                            import tempfile
                            import soundfile as sf
                            start_time = time.time()
                            try:
                                with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_file:
                                    sf.write(temp_file.name, normalized_audio, self.config.RATE)
                                    result = self.lightning_model.transcribe(audio_path=temp_file.name)
                                import os
                                os.unlink(temp_file.name)
                                processing_time = time.time() - start_time
                                if self.args.debug:
                                    print(f"âš¡ Lightning Whisperå‡¦ç†æ™‚é–“: {processing_time:.2f}ç§’")
                            except Exception as lightning_error:
                                print(f"âš ï¸ Lightning Whisperå‡¦ç†ã‚¨ãƒ©ãƒ¼: {lightning_error}")
                                # æ¨™æº–mlx-whisperã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                                start_time = time.time()
                                model_repo = getattr(self.args, 'model_path', None) or f"mlx-community/whisper-{self.args.model_size}-mlx"
                                result = mlx_whisper.transcribe(normalized_audio,
                                                                language=self.lang_config.source_lang,
                                                                path_or_hf_repo=model_repo,
                                                                verbose=False
                                )
                                processing_time = time.time() - start_time
                                if self.args.debug:
                                    print(f"âš¡ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯MLX Whisperå‡¦ç†æ™‚é–“: {processing_time:.2f}ç§’")
                        else:
                            # æ¨™æº–mlx-whisperï¼ˆ3-4å€é«˜é€ŸåŒ–ï¼‰
                            start_time = time.time()
                            model_repo = getattr(self.args, 'model_path', None) or f"mlx-community/whisper-{self.args.model_size}-mlx"
                            result = mlx_whisper.transcribe(normalized_audio,
                                                            language=self.lang_config.source_lang,
                                                            path_or_hf_repo=model_repo,
                                                            verbose=False  # é«˜é€ŸåŒ–ã®ãŸã‚è©³ç´°å‡ºåŠ›ã‚’ç„¡åŠ¹åŒ–
                            )
                            processing_time = time.time() - start_time
                            if self.args.debug:
                                print(f"âš¡ MLX Whisperå‡¦ç†æ™‚é–“: {processing_time:.2f}ç§’")
                    else:
                        result = self.model.transcribe(normalized_audio,
                                                       language=self.lang_config.source_lang
                        )
                except Exception as e:
                    print(f"éŸ³å£°èªè­˜ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
                
                # no_speech_probãƒã‚§ãƒƒã‚¯ï¼ˆå¹»è´é˜²æ­¢ï¼‰
                no_speech_prob = 0.0
                if 'segments' in result and len(result['segments']) > 0:
                    no_speech_prob = result['segments'][0].get('no_speech_prob', 0.0)
                
                # ç„¡éŸ³ç¢ºç‡ãŒé«˜ã„å ´åˆã¯å¹»è´ã¨ã—ã¦ç ´æ£„
                NO_SPEECH_THRESHOLD = 0.5  # 50%ä»¥ä¸Šã®ç„¡éŸ³ç¢ºç‡ã§ç ´æ£„
                if no_speech_prob > NO_SPEECH_THRESHOLD:
                    if self.args.debug:
                        print(f"ğŸš« å¹»è´æ¤œå‡º: ç„¡éŸ³ç¢ºç‡ {no_speech_prob:.2f} > é–¾å€¤ {NO_SPEECH_THRESHOLD}")
                    continue
                
                text = result['text'].strip()
                
                current_time = time.time()
                if text and (text != last_text or current_time - last_text_time > 5):
                    self.print_with_strictly_controlled_linebreaks(text)
                    last_text = text
                    last_text_time = current_time
                    if self.translation_queue:
                        self.translation_queue.put(text)
                    # èªè­˜çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«è¿½è¨˜
                    with open(self.log_file_path, "a", encoding="utf-8") as log_file:
                        log_file.write(text + "\n")

                elif self.args.debug:
                    print("å‡¦ç†å¾Œã®ãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã‹ã€ç›´å‰ã®æ–‡ã¨åŒã˜ãŸã‚å‡ºåŠ›ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")

            except queue.Empty:
                if self.args.debug:
                    print("èªè­˜ã‚­ãƒ¥ãƒ¼ãŒç©ºã§ã™")
            except Exception as e:
                print(f"\nã‚¨ãƒ©ãƒ¼ (èªè­˜ã‚¹ãƒ¬ãƒƒãƒ‰): {e}", flush=True)

    def normalize_audio(self, audio_data):
        if self.config.FORMAT == pyaudio.paFloat32:
            return np.clip(audio_data, -1.0, 1.0)
        elif self.config.FORMAT == pyaudio.paInt8:
            return audio_data.astype(np.float32) / 128.0
        elif self.config.FORMAT == pyaudio.paInt16:
            return audio_data.astype(np.float32) / 32768.0
        elif self.config.FORMAT == pyaudio.paInt32:
            return audio_data.astype(np.float32) / 2147483648.0
        else:
            raise ValueError(f"Unsupported audio format: {self.config.FORMAT}")

    def save_audio_debug(self, audio_data, filename):
        with wave.open(filename, 'wb') as wf:
            wf.setnchannels(self.config.CHANNELS)
            wf.setsampwidth(pyaudio.get_sample_size(self.config.FORMAT))
            wf.setframerate(self.config.RATE)
            wf.writeframes(audio_data.tobytes())

    @staticmethod
    def is_sentence_end(word):
        # æ—¥æœ¬èªã¨è‹±èªã®æ–‡æœ«è¨˜å·
        sentence_end_chars = ('.', '!', '?', 'ã€‚', 'ï¼', 'ï¼Ÿ')
        return word.endswith(sentence_end_chars)

    def print_with_strictly_controlled_linebreaks(self, text):
        words = text.split()
        buffer = []
        final_output = ""
        for i, word in enumerate(words):
            buffer.append(word)
            
            if SpeechRecognition.is_sentence_end(word) or i == len(words) - 1:
                line = ' '.join(buffer)
                final_output += line
                if SpeechRecognition.is_sentence_end(word):
                    final_output += '\n'
                elif i == len(words) - 1:
                    final_output += ' '
                buffer = []

        if buffer:
            line = ' '.join(buffer)
            final_output += line

        # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«å‡ºåŠ›
        print(final_output, end='', flush=True)

