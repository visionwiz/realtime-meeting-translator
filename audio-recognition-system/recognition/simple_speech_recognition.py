import os
import queue
import threading
import time
from typing import Callable
from google.cloud import speech_v2
from google.api_core.client_options import ClientOptions

class SimpleStreamingSpeechRecognition:
    """Google Cloud Speech-to-Text V2 + chirp_2ã®çœŸã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å®Ÿè£…ï¼ˆå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå®Œå…¨æº–æ‹ ï¼‰"""
    
    def __init__(self, language_code="ja-JP", result_callback=None, 
                 project_id=None, region="asia-southeast1", verbose=False):
        # åŸºæœ¬è¨­å®š
        self.language_code = language_code
        self.result_callback = result_callback
        self.project_id = project_id or os.getenv("GOOGLE_CLOUD_PROJECT", "meeting-voice-bridge")
        self.region = region
        self.verbose = verbose  # è©³ç´°ãƒ­ã‚°åˆ¶å¾¡
        
        # ãƒ­ã‚°åˆ¶å¾¡ç”¨
        self.last_audio_log_time = 0
        self.last_response_log_time = 0
        self.audio_log_interval = 2.0  # 2ç§’é–“éš”ã§ãƒ­ã‚°
        self.response_count = 0
        
        # Google Cloud Speech V2ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
        self.client_options = ClientOptions(
            api_endpoint=f"{self.region}-speech.googleapis.com"
        )
        self.client = speech_v2.SpeechClient(client_options=self.client_options)
        
        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç®¡ç†
        self.audio_queue = queue.Queue()
        self.streaming_active = False
        self.streaming_start_time = None
        self.max_streaming_duration = 300  # 5åˆ†åˆ¶é™
        
        print(f"ğŸŒ©ï¸ Simple Google Cloud Speech-to-Text V2 + chirp_2 åˆæœŸåŒ–ï¼ˆçœŸã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç‰ˆï¼‰")
        print(f"   ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: {self.project_id}")
        print(f"   ãƒªãƒ¼ã‚¸ãƒ§ãƒ³: {self.region}")
        print(f"   è¨€èª: {language_code}")
        if not self.verbose:
            print("   ãƒ­ã‚°ãƒ¢ãƒ¼ãƒ‰: ç°¡æ½”è¡¨ç¤ºï¼ˆæœ€çµ‚çµæœã®ã¿è¡¨ç¤ºã€è©³ç´°ãƒ­ã‚°ã¯verbose=Trueã§æœ‰åŠ¹åŒ–ï¼‰")
    
    def add_audio_data(self, audio_data: bytes):
        """éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ """
        if self.streaming_active:
            self.audio_queue.put(audio_data)
            
            # ãƒ­ã‚°å‡ºåŠ›é »åº¦åˆ¶å¾¡ï¼ˆverboseãƒ¢ãƒ¼ãƒ‰ã®ã¿è©³ç´°è¡¨ç¤ºï¼‰
            if self.verbose:
                current_time = time.time()
                if (current_time - self.last_audio_log_time) > self.audio_log_interval:
                    print(f"ğŸ¤ éŸ³å£°ãƒ‡ãƒ¼ã‚¿è¿½åŠ : {len(audio_data)} bytesï¼ˆã‚­ãƒ¥ãƒ¼ã‚µã‚¤ã‚º: {self.audio_queue.qsize()}ï¼‰")
                    self.last_audio_log_time = current_time
    
    def start_streaming_recognition(self):
        """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°èªè­˜é–‹å§‹"""
        self.streaming_active = True
        self.streaming_start_time = time.time()
        
        # åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§èªè­˜å®Ÿè¡Œ
        recognition_thread = threading.Thread(target=self._run_streaming_recognition)
        recognition_thread.daemon = True
        recognition_thread.start()
        
        print("ğŸŒ©ï¸ çœŸã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°èªè­˜é–‹å§‹ï¼ˆå…¬å¼æº–æ‹ ç‰ˆï¼‰")
    
    def _audio_generator(self):
        """å…¬å¼æº–æ‹ ã®éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ï¼ˆç¶™ç¶šçš„ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰"""
        if self.verbose:
            print("ğŸµ éŸ³å£°ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼é–‹å§‹")
        
        while self.streaming_active:
            try:
                # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æ™‚é–“åˆ¶é™ãƒã‚§ãƒƒã‚¯
                if self.streaming_start_time and (time.time() - self.streaming_start_time) > self.max_streaming_duration:
                    print("â° ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æ™‚é–“åˆ¶é™ï¼ˆ5åˆ†ï¼‰ã«é”ã—ã¾ã—ãŸã€‚æ¥ç¶šã‚’å†é–‹ã—ã¾ã™ã€‚")
                    break
                
                # ãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°å–å¾—ã§ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§ã‚’ç¢ºä¿
                audio_data = self.audio_queue.get(timeout=1.0)
                if audio_data is None:  # çµ‚äº†ã‚·ã‚°ãƒŠãƒ«
                    if self.verbose:
                        print("ğŸ›‘ éŸ³å£°ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼çµ‚äº†ã‚·ã‚°ãƒŠãƒ«å—ä¿¡")
                    break
                    
                if self.verbose:
                    print(f"ğŸ¶ éŸ³å£°ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ: {len(audio_data)} bytes")
                yield audio_data
                
            except queue.Empty:
                # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚ã¯ç¶™ç¶šï¼ˆéŸ³å£°ãŒãªã„é–“ã‚‚æ¥ç¶šç¶­æŒï¼‰
                if self.verbose:
                    print("â° éŸ³å£°å¾…æ©Ÿä¸­...")
                continue
        
        if self.verbose:
            print("ğŸµ éŸ³å£°ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼çµ‚äº†")
    
    def _run_streaming_recognition(self):
        """çœŸã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°èªè­˜å‡¦ç†ï¼ˆå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå®Œå…¨æº–æ‹ ï¼‰"""
        try:
            # Recognizer ãƒªã‚½ãƒ¼ã‚¹ãƒ‘ã‚¹
            recognizer_name = f"projects/{self.project_id}/locations/{self.region}/recognizers/_"
            if self.verbose:
                print(f"ğŸ”§ Recognizer: {recognizer_name}")
            
            # èªè­˜è¨­å®šï¼ˆæ˜ç¤ºçš„PCMãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæŒ‡å®šï¼‰
            recognition_config = speech_v2.types.RecognitionConfig(
                explicit_decoding_config=speech_v2.types.ExplicitDecodingConfig(
                    encoding=speech_v2.types.ExplicitDecodingConfig.AudioEncoding.LINEAR16,
                    sample_rate_hertz=16000,
                    audio_channel_count=1,
                ),
                language_codes=[self.language_code],
                model="chirp_2",
                features=speech_v2.types.RecognitionFeatures(
                    enable_automatic_punctuation=True,
                    enable_word_time_offsets=True,
                )
            )
            
            streaming_config = speech_v2.types.StreamingRecognitionConfig(
                config=recognition_config,
                streaming_features=speech_v2.types.StreamingRecognitionFeatures(
                    interim_results=True  # ä¸­é–“çµæœã‚‚å—ä¿¡
                )
            )
            
            # å…¬å¼æº–æ‹ ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆç”Ÿæˆé–¢æ•°
            def generate_requests():
                """å…¬å¼æº–æ‹ ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼"""
                # æœ€åˆã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼ˆè¨­å®šã®ã¿ï¼‰
                print("ğŸ“¤ è¨­å®šãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡")
                yield speech_v2.types.StreamingRecognizeRequest(
                    recognizer=recognizer_name,
                    streaming_config=streaming_config
                )
                
                # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼ˆç¶™ç¶šçš„ï¼‰
                print("ğŸµ éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é–‹å§‹")
                for audio_data in self._audio_generator():
                    if self.verbose:
                        print(f"ğŸ“¤ éŸ³å£°ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡: {len(audio_data)} bytes")
                    yield speech_v2.types.StreamingRecognizeRequest(
                        audio=audio_data
                    )
                
                if self.verbose:
                    print("ğŸ“¤ ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼çµ‚äº†")
            
            print("ğŸš€ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°èªè­˜å®Ÿè¡Œé–‹å§‹...")
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°èªè­˜å®Ÿè¡Œï¼ˆå…¬å¼æº–æ‹ ï¼‰
            response_stream = self.client.streaming_recognize(
                requests=generate_requests()
            )
            
            print("ğŸ“¨ ãƒ¬ã‚¹ãƒãƒ³ã‚¹å‡¦ç†é–‹å§‹...")
            self.response_count = 0
            
            try:
                for response in response_stream:
                    self.response_count += 1
                    
                    # ãƒ¬ã‚¹ãƒãƒ³ã‚¹å—ä¿¡ãƒ­ã‚°ï¼ˆverboseãƒ¢ãƒ¼ãƒ‰ã®ã¿ï¼‰
                    if self.verbose:
                        current_time = time.time()
                        if (current_time - self.last_response_log_time) > 1.0:
                            print(f"ğŸ“¥ ãƒ¬ã‚¹ãƒãƒ³ã‚¹å—ä¿¡ #{self.response_count}")
                            self.last_response_log_time = current_time
                    
                    if hasattr(response, 'results') and response.results:
                        for i, result in enumerate(response.results):
                            if self.verbose:
                                print(f"ğŸ¯ çµæœ #{i}: is_final={result.is_final}")
                            
                            if hasattr(result, 'alternatives') and result.alternatives:
                                # æœ€åˆã®ä»£æ›¿çµæœã‚’ä½¿ç”¨
                                transcript = result.alternatives[0].transcript
                                confidence = getattr(result.alternatives[0], 'confidence', 0.0)
                                is_final = result.is_final
                                
                                if transcript.strip():
                                    # æœ€çµ‚çµæœã®ã¿è¡¨ç¤ºï¼ˆé€”ä¸­çµæœã¯éè¡¨ç¤ºï¼‰
                                    if is_final:
                                        print(f"\nğŸ¯ æœ€çµ‚çµæœ: {transcript}")
                                        if self.verbose:
                                            print(f"   ä¿¡é ¼åº¦: {confidence:.2f}")
                                    elif self.verbose:
                                        # verboseãƒ¢ãƒ¼ãƒ‰ã§ã®ã¿é€”ä¸­çµæœã‚’è¡¨ç¤º
                                        print(f"ğŸ“ é€”ä¸­çµæœ: {transcript}")
                                    
                                    # çµæœã‚’ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°ã«é€ä¿¡ï¼ˆãƒ­ã‚°è¡¨ç¤ºã¯ã—ãªã„ï¼‰
                                    if self.result_callback:
                                        try:
                                            self.result_callback(transcript, confidence, is_final)
                                        except Exception as callback_error:
                                            print(f"\nâŒ ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é€ä¿¡ã‚¨ãƒ©ãƒ¼: {callback_error}")
                                elif self.verbose:
                                    print("ğŸ“ ç©ºã®transcript")
                            elif self.verbose:
                                print("ğŸ“ alternatives ãªã—")
                    elif self.verbose:
                        print("ğŸ“­ results ãªã—")
                        # ã‚¨ãƒ©ãƒ¼æƒ…å ±ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                        if hasattr(response, 'error'):
                            print(f"âŒ ã‚¨ãƒ©ãƒ¼æƒ…å ±: {response.error}")
            
            except Exception as response_error:
                print(f"âŒ ãƒ¬ã‚¹ãƒãƒ³ã‚¹å‡¦ç†ä¸­ã‚¨ãƒ©ãƒ¼: {response_error}")
                if self.verbose:
                    import traceback
                    traceback.print_exc()
                            
        except Exception as e:
            print(f"âš ï¸ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°èªè­˜ã‚¨ãƒ©ãƒ¼: {e}")
            if self.verbose:
                import traceback
                traceback.print_exc()
        finally:
            self.streaming_active = False
            print("ğŸŒ©ï¸ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°èªè­˜çµ‚äº†")
    
    def stop_recognition(self):
        """èªè­˜åœæ­¢"""
        self.streaming_active = False
        self.audio_queue.put(None)  # çµ‚äº†ã‚·ã‚°ãƒŠãƒ«
        print("ğŸ›‘ èªè­˜åœæ­¢è¦æ±‚é€ä¿¡")
    
    def is_active(self):
        """èªè­˜ãŒã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‹ã©ã†ã‹"""
        return self.streaming_active 